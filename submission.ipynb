{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c9a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9221\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 50\n",
      "[LightGBM] [Info] Start training from score 0.510260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-25 19:13:40,586] A new study created in memory with name: no-name-f888e924-4b05-4522-94f7-d29fec539ff5\n",
      "[I 2025-06-25 19:14:20,033] Trial 0 finished with value: 0.10151686121516494 and parameters: {'alpha': 3.8724086751382454}. Best is trial 0 with value: 0.10151686121516494.\n",
      "[I 2025-06-25 19:14:58,270] Trial 1 finished with value: 0.10146375715685002 and parameters: {'alpha': 1.433045871631419}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:15:35,394] Trial 2 finished with value: 0.10155181346517078 and parameters: {'alpha': 5.763086634012815}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:16:14,671] Trial 3 finished with value: 0.10155651364396334 and parameters: {'alpha': 6.036615627937549}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:16:51,055] Trial 4 finished with value: 0.1015808779556222 and parameters: {'alpha': 7.5340993175945385}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:17:25,921] Trial 5 finished with value: 0.1015238894484677 and parameters: {'alpha': 4.233076970900662}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:18:03,381] Trial 6 finished with value: 0.10153281507354443 and parameters: {'alpha': 4.704951368864941}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:18:43,402] Trial 7 finished with value: 0.1015335419347433 and parameters: {'alpha': 4.744073717219644}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:19:26,547] Trial 8 finished with value: 0.10150154687419201 and parameters: {'alpha': 3.1186100681949727}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:20:02,672] Trial 9 finished with value: 0.10149210529026315 and parameters: {'alpha': 2.6750159952641943}. Best is trial 1 with value: 0.10146375715685002.\n",
      "[I 2025-06-25 19:20:39,132] Trial 10 finished with value: 0.10143370360882525 and parameters: {'alpha': 0.21319171501661094}. Best is trial 10 with value: 0.10143370360882525.\n",
      "[I 2025-06-25 19:21:16,926] Trial 11 finished with value: 0.10143361797245626 and parameters: {'alpha': 0.2095295751869959}. Best is trial 11 with value: 0.10143361797245626.\n",
      "[I 2025-06-25 19:21:53,676] Trial 12 finished with value: 0.10143870452351501 and parameters: {'alpha': 0.41996790402614304}. Best is trial 11 with value: 0.10143361797245626.\n",
      "[I 2025-06-25 19:22:33,988] Trial 13 finished with value: 0.10143321029577188 and parameters: {'alpha': 0.19200763299440854}. Best is trial 13 with value: 0.10143321029577188.\n",
      "[I 2025-06-25 19:23:17,872] Trial 14 finished with value: 0.10161248904635498 and parameters: {'alpha': 9.694204239697632}. Best is trial 13 with value: 0.10143321029577188.\n",
      "[I 2025-06-25 19:24:01,478] Trial 15 finished with value: 0.1014725148510631 and parameters: {'alpha': 1.8031291066119552}. Best is trial 13 with value: 0.10143321029577188.\n",
      "[I 2025-06-25 19:24:49,149] Trial 16 finished with value: 0.1014652148115062 and parameters: {'alpha': 1.493867247846385}. Best is trial 13 with value: 0.10143321029577188.\n",
      "[I 2025-06-25 19:25:28,993] Trial 17 finished with value: 0.10143186034062662 and parameters: {'alpha': 0.13272859651976426}. Best is trial 17 with value: 0.10143186034062662.\n",
      "[I 2025-06-25 19:26:12,296] Trial 18 finished with value: 0.10148869686232949 and parameters: {'alpha': 2.518705380555828}. Best is trial 17 with value: 0.10143186034062662.\n",
      "[I 2025-06-25 19:26:53,234] Trial 19 finished with value: 0.10157638937499196 and parameters: {'alpha': 7.2478477282894405}. Best is trial 17 with value: 0.10143186034062662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ultra-optimized pipeline complete. Predictions saved to final_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression, RFE\n",
    "from sklearn.inspection import permutation_importance\n",
    "from packaging import version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "import shap\n",
    "import optuna\n",
    "import sklearn\n",
    "\n",
    "# -------- Step 1: Load Data --------\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train = train_df.drop(columns=['efficiency'])\n",
    "y_train = train_df['efficiency']\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# -------- Step 2: Feature Engineering --------\n",
    "class SolarFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        numeric_cols = ['voltage', 'current', 'irradiance', 'temperature', 'wind_speed', \n",
    "                        'soiling_ratio', 'module_temperature', 'panel_age', 'maintenance_count',\n",
    "                        'humidity', 'pressure', 'cloud_coverage']\n",
    "        for col in numeric_cols:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "        for col in numeric_cols:\n",
    "            if col == 'maintenance_count':\n",
    "                X[col] = X[col].fillna(0)\n",
    "            else:\n",
    "                X[col] = X[col].fillna(X[col].mean())\n",
    "        X['power_output'] = X['voltage'] * X['current']\n",
    "        X['irradiance_temp_ratio'] = X['irradiance'] / (X['temperature'] + 1)\n",
    "        X['cooling_effect'] = X['wind_speed'] / (X['temperature'] + 1)\n",
    "        X['efficiency_loss_due_to_soiling'] = X['soiling_ratio'] * X['irradiance']\n",
    "        X['temp_diff'] = X['module_temperature'] - X['temperature']\n",
    "        X['age_per_maintenance'] = X['panel_age'] / (X['maintenance_count'] + 1)\n",
    "        X['humidity_pressure_interaction'] = X['humidity'] / (X['pressure'] + 1)\n",
    "        X['cloud_temp_impact'] = X['cloud_coverage'] * X['module_temperature']\n",
    "        X['expected_efficiency'] = (X['irradiance'] * (1 - X['soiling_ratio'])) / (X['module_temperature'] + 1)\n",
    "        X['is_soiled'] = (X['soiling_ratio'] > 0.5).astype(int)\n",
    "        X['is_cloudy'] = (X['cloud_coverage'] > 70).astype(int)\n",
    "        X['high_panel_age'] = (X['panel_age'] > 8).astype(int)\n",
    "        return X\n",
    "\n",
    "fe = SolarFeatureEngineer()\n",
    "X_train_fe = fe.fit_transform(X_train)\n",
    "X_test_fe = fe.transform(X_test)\n",
    "\n",
    "# -------- Step 3: Preprocessing Pipelines --------\n",
    "numeric_cols = X_train_fe.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = ['string_id', 'error_code', 'installation_type']\n",
    "\n",
    "if version.parse(sklearn_version) >= version.parse(\"1.2\"):\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "else:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', ohe)\n",
    "])\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "X_train_processed = full_pipeline.fit_transform(X_train_fe)\n",
    "X_test_processed = full_pipeline.transform(X_test_fe)\n",
    "\n",
    "# -------- Step 4: Polynomial Features --------\n",
    "top_feats = ['irradiance', 'temperature', 'humidity', 'soiling_ratio', 'module_temperature']\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_poly_train = poly.fit_transform(X_train_fe[top_feats])\n",
    "X_poly_test = poly.transform(X_test_fe[top_feats])\n",
    "\n",
    "X_train_combined = np.hstack([X_train_processed, X_poly_train])\n",
    "X_test_combined = np.hstack([X_test_processed, X_poly_test])\n",
    "\n",
    "# -------- Step 5: Enhanced Feature Selection --------\n",
    "shap_model = XGBRegressor(n_estimators=200, learning_rate=0.1, verbosity=0, random_state=42)\n",
    "shap_model.fit(X_train_combined, y_train)\n",
    "\n",
    "# SHAP values\n",
    "explainer = shap.Explainer(shap_model)\n",
    "X_sample = X_train_combined[:100]\n",
    "shap_values = explainer(X_sample)\n",
    "shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "\n",
    "# Permutation Importance\n",
    "perm = permutation_importance(shap_model, X_train_combined, y_train, scoring='neg_root_mean_squared_error', n_repeats=5, n_jobs=-1)\n",
    "perm_importance = np.abs(perm.importances_mean)\n",
    "\n",
    "# Mutual Information\n",
    "mi_importance = mutual_info_regression(X_train_combined, y_train, random_state=42)\n",
    "\n",
    "# LGBM Importance\n",
    "lgb = LGBMRegressor(n_estimators=200, random_state=42)\n",
    "lgb.fit(X_train_combined, y_train)\n",
    "lgb_importance = lgb.feature_importances_\n",
    "\n",
    "# CatBoost Importance\n",
    "cat = CatBoostRegressor(iterations=200, verbose=0, random_state=42)\n",
    "cat.fit(X_train_combined, y_train)\n",
    "cat_importance = cat.get_feature_importance()\n",
    "\n",
    "# SHAP Interaction\n",
    "interaction_values = explainer.shap_interaction_values(X_sample)\n",
    "interaction_importance = np.abs(interaction_values).mean(axis=(0, 1))\n",
    "\n",
    "# Average all importances\n",
    "combined_importance = (\n",
    "    shap_importance + perm_importance + mi_importance + lgb_importance + cat_importance + interaction_importance\n",
    ") / 6\n",
    "\n",
    "# Select top features\n",
    "top_k = 60\n",
    "top_indices = np.argsort(combined_importance)[-top_k:]\n",
    "\n",
    "X_train_selected = X_train_combined[:, top_indices]\n",
    "X_test_selected = X_test_combined[:, top_indices]\n",
    "\n",
    "# Optional: Refine with RFE\n",
    "rfe = RFE(Ridge(alpha=1.0), n_features_to_select=40, step=10)\n",
    "rfe.fit(X_train_selected, y_train)\n",
    "X_train_selected = X_train_selected[:, rfe.support_]\n",
    "X_test_selected = X_test_selected[:, rfe.support_]\n",
    "\n",
    "# -------- Step 6: Optuna Tuning --------\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.1, 10.0)\n",
    "    model = StackingRegressor(\n",
    "        estimators=[\n",
    "            ('xgb', XGBRegressor(n_estimators=500, learning_rate=0.05, verbosity=0, random_state=42)),\n",
    "            ('lgb', LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42)),\n",
    "            ('cat', CatBoostRegressor(iterations=500, learning_rate=0.05, verbose=0, random_state=42)),\n",
    "        ],\n",
    "        final_estimator=Ridge(alpha=alpha),\n",
    "        passthrough=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    preds = model.predict(X_train_selected)\n",
    "    rmse = np.sqrt(np.mean((y_train - preds) ** 2))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "best_alpha = study.best_params['alpha']\n",
    "\n",
    "# -------- Step 7: Final Model --------\n",
    "final_model = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('xgb', XGBRegressor(n_estimators=500, learning_rate=0.05, verbosity=0, random_state=42)),\n",
    "        ('lgb', LGBMRegressor(n_estimators=500, learning_rate=0.05, random_state=42)),\n",
    "        ('cat', CatBoostRegressor(iterations=500, learning_rate=0.05, verbose=0, random_state=42)),\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=best_alpha),\n",
    "    passthrough=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# -------- Step 8: Predict and Save --------\n",
    "y_pred_test = final_model.predict(X_test_selected)\n",
    "output_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'efficiency': y_pred_test\n",
    "})\n",
    "output_df.to_csv(\"final_predictions.csv\", index=False)\n",
    "print(\"âœ… Ultra-optimized pipeline complete. Predictions saved to final_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
